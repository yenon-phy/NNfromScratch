{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from common.layers import Affine\n",
    "from common.layers import Sigmoid\n",
    "from common.layers import SoftmaxWithLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, n_features, n_output, n_hidden=30, l2=0.0, l1=0.0, \\\n",
    "                 epochs=50, eta=0.001, decrease_const=0.0, shuffle=True, \\\n",
    "                 n_minibatches=1, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.l2 = l2\n",
    "        self.l1 = l1\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.n_minibatches = n_minibatches\n",
    "        \n",
    "        self.params = {}\n",
    "        self._init_weights()\n",
    "        \n",
    "        self.layers = {}\n",
    "        self.layers['Affine_1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Sigmoid'] = Sigmoid()\n",
    "        self.layers['Affine_2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        self._loss = []\n",
    "        self._iter_t = 0\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        ls_nodes = [self.n_features, self.n_hidden, self.n_output]\n",
    "        scale_1 = np.sqrt(1.0 / ls_nodes[0])\n",
    "        scale_2 = np.sqrt(1.0 / ls_nodes[1])\n",
    "  \n",
    "        self.params['W1'] = scale_1 * np.random.randn(ls_nodes[0], ls_nodes[1])\n",
    "        self.params['b1'] = np.zeros(ls_nodes[1])   \n",
    "        self.params['W2'] = scale_2 * np.random.randn(ls_nodes[1], ls_nodes[2])\n",
    "        self.params['b2'] = np.zeros(ls_nodes[2])  \n",
    "        \n",
    "    def predict(self, X):\n",
    "        for layer in self.layers.values():\n",
    "            X = layer(X)\n",
    "        y_hat = X\n",
    "        return y_hat\n",
    "\n",
    "    def _calc_loss(self, X, t):\n",
    "        y_hat = self.predict(X)\n",
    "\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        l2_term, l1_term = 0.0, 0.0\n",
    "        l2_term += 0.5 * self.l2 * np.sum(W1 ** 2)\n",
    "        l2_term += 0.5 * self.l2 * np.sum(W2 ** 2)\n",
    "        l1_term += 0.5 * self.l1 * np.abs(W1).sum()\n",
    "        l1_term += 0.5 * self.l1 * np.abs(W2).sum()\n",
    "        \n",
    "        loss = self.last_layer(y_hat, t) + l2_term + l1_term\n",
    "        return loss\n",
    "\n",
    "    def accuracy(self, X, t):\n",
    "        y_hat = self.predict(X)\n",
    "        y = np.argmax(y_hat, axis=1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(X.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def _encode_labels(self, y, n_labels):\n",
    "        onehot = np.zeros((y.shape[0], n_labels))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[idx, val] = 1.0\n",
    "        return onehot\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y, self.n_output)\n",
    "        \n",
    "        self._loss = []\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress:\n",
    "                sys.stderr.write('\\rEpoch: {}/{}'.format(i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[idx]\n",
    "\n",
    "            batches = np.array_split(range(y_data.shape[0]), self.n_minibatches)\n",
    "            self._iter_t = 0\n",
    "            for batch in batches:\n",
    "                # forward\n",
    "                X_batch, y_batch = X_data[batch], y_enc[batch]\n",
    "                loss = self._calc_loss(X_batch, y_batch)\n",
    "                self._loss.append(loss)\n",
    "                \n",
    "                # backward\n",
    "                delta = 1\n",
    "                delta = self.last_layer.backward(delta)\n",
    "\n",
    "                layers = list(self.layers.values())\n",
    "                layers.reverse()\n",
    "                for layer in layers:\n",
    "                    delta = layer.backward(delta)\n",
    "\n",
    "                # gradients\n",
    "                grads = {}\n",
    "                W1 = self.layers['Affine_1'].W\n",
    "                W2 = self.layers['Affine_2'].W\n",
    "                \n",
    "                grads['W1'] = self.layers['Affine_1'].dW \\\n",
    "                + self.l2 * W1 \\\n",
    "                + self.l1 * np.sign(W1)\n",
    "                grads['b1'] = self.layers['Affine_1'].db\n",
    "\n",
    "                grads['W2'] = self.layers['Affine_2'].dW \\\n",
    "                + self.l2 * W2 \\\n",
    "                + self.l1 * np.sign(W2)\n",
    "                grads['b2'] = self.layers['Affine_2'].db\n",
    "                \n",
    "                self._update_grads(self.params, grads)\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    \"\"\"\n",
    "    # SGD\n",
    "    def _update_grads(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.eta * grads[key] \n",
    "            \n",
    "    \"\"\"    \n",
    "    \n",
    "    # Adam\n",
    "    def _update_grads(self, params, grads):\n",
    "        beta1, beta2 = 0.9, 0.999\n",
    "        eps = 1e-8\n",
    "        m, v = {}, {}\n",
    "        for key, val in params.items():\n",
    "            m[key] = np.zeros_like(val)\n",
    "            v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self._iter_t += 1\n",
    "        for key in params.keys():\n",
    "            t = self._iter_t\n",
    "            # (1 - beta)で因数分解されたAdamの更新式\n",
    "            m[key] += (1 - beta1) * (grads[key] + m[key])\n",
    "            m[key] /= 1 - beta1 ** t\n",
    "            v[key] += (1 - beta2) * (grads[key]**2 + v[key])\n",
    "            v[key] /= 1 - beta2 ** t\n",
    "            \n",
    "            params[key] -= self.eta * m[key] / (np.sqrt(v[key]) + eps) \n",
    "    \n",
    "    @property\n",
    "    def loss_(self):\n",
    "        return self._loss\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
